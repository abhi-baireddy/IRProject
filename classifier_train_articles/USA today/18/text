As the coronavirus epidemic eats into Facebook and Instagram's roster of employees who monitor reported posts, inappropriate posts are being ignored and left up, despite users' protests.

With children out of school and potentially spending more time online, how can parents protect them as the apps reduce their self-policing?

Sean Wright, founder and president of Affinity Technology Partners, a managed IT services provider in Brentwood, said the best way to protect children from inappropriate content is to discuss online safety with them and let them know it's OK to ask questions about what they see online.

He recommends that parents consider installing apps and programs that limit children's screen time and can censor content on multiple devices, especially since 95% of teenagers now have access to a smartphone, the Pew Research Center says.

“Nowadays it’s even more important to have those conversations because screen time is even more now," said Wright, who is a parent of four children.

Facebook has relaxed content moderation

Since mid-March, Facebook and Instagram, which share the same content moderator workforce, have reduced the number of employees who examine reported content. With fewer employees dedicated to investigating posts, many reported posts have been left unchecked.

At least one inappropriate photo, of a woman's genitalia, has circulated in a Nashville Facebook group and remained up days after it was reported.

When a user reported the photo, he received a message from Facebook stating that the complaint wasn't a priority because of the coronavirus.

According to the site's standards, nudity, including when genitalia is visible, is prohibited.

"When people report content to us, we are now ​letting them know that we will prioritize those instances that have the greatest potential ​to harm our community so we can keep our reviewers safe by having them stay home," a Facebook company spokesperson said by email. "With fewer reviewers available, we will increase the use of AI to proactively remove content that violates our Community Standards."

The spokesperson would not provide metrics on how long it takes the site to remove a flagged post.

Children exposed to concerning content online

Since switching to remote work, Facebook and Instagram employees have focused on removing content related to child safety, terrorism, suicide and self-injury, and harmful content related to COVID-19, the company said.

A Pew Research Center study shows that 59% of U.S. teens have been bullied online, including name calling and receiving explicit images. Surveyed students pointed fingers at social media companies, and 66% said the platforms weren't doing anything to curb bullying.

According to a study performed by Bark Technologies, which created an app that monitors content, almost 90% of children and teens have either expressed or experienced violent subject matter or thought.

The app analyzed more than 873.8 million messages and found that up to 84% of teens have been exposed to sexual content.

Wright said that having installed software to monitor content can act as a potential backup if sites don't flag inappropriate content automatically. He personally recommends Covenant Eyes, which can work on any device – Apple, PC or Android – and Bark.

The Bark app, which works on a multitude of social platforms, including Instagram, Snapchat and even text messages, analyzes messages and feeds in search of content. Through its algorithm, it can flag to parents certain threads, like ones that discuss bullying, self-harm, drugs and alcohol, or sexual content.

Contributing: Jazmin Goodwin, USA TODAY.

Follow Brinley Hineman on Twitter @brinleyhineman.